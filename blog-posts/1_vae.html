<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Research Lab, Home, Velit Aliquet Sagittis University"
    />
    <meta name="author" content="" />
    <!-- use root domain folder to load the css inside subfolders -->
    <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
    <link
      href="/css/bootstrap-responsive.min.css"
      rel="stylesheet"
      type="text/css"
    />
    <link href="/css/theme.css" rel="stylesheet" type="text/css" />
    <link rel="stylesheet" href="/css/blog_style.css" type="text/css" />
    <script src="/js/jquery-1.9.1.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
      integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    />
    <!-- MathJAX -->
    <script type="text/x-mathjax-config;executed=false">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$','$$'], ['\\[', '\\]']],
          processEscapes: true,
        }
      });
    </script>
    <script
      type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>
    <script>
      $(function () {
        $("#header").load("/header.html");
        $("#footer").load("/footer.html");
      });
    </script>
  </head>

  <body>
    <script src="/js/web-components/NavBar.js"></script>
    <div id="header"></div>
    <nav-bar></nav-bar>

    <div class="container">
      <div class="row-fluid">
        <h1>Variational Auto Encoder</h1>
        <span class="post-author">
          <i class="fa fa-user" aria-hidden="true"></i>
          Fahim Faisal Niloy
        </span>
        <br /><br />
        <h3>Introduction</h3>
        <br />
        <p>
          In this document, we will address two fundamental problems in machine
          learn-ing,
        </p>
        <ul>
          <li>
            Density estimation: Modeling $p(x)$. This distribution helps us to
            quantify how probable a new data point is.
          </li>
          <li>
            Classification: Modeling the distribution $p(y|x)$, which gives us
            the class distribution given the input data distribution
          </li>
        </ul>
        <p>
          At first, we will look into the first problem, that is modeling the
          data gen- erating process.
        </p>
        <br />
        <h3>Density Estimation</h3>
        <br />
        <p>
          $p_\theta(x)$ is comparatively easier to model if all the variables of
          the system are observed in data. However, if there are latent
          variables associated in the system, modeling $p(x)$ is difficult
          because the latent variables have to be taken into account while
          modeling $p_\theta(x)$ with the following equation.
        </p>
        <span> $$ p_\theta (x) = \int{p_\theta(x,z)dz} \tag{1} $$ </span>
        <p>
          The distribution $p_\theta(x,z)$ can be assumed by making suitable
          choices for $p_\theta(z)$ and $p_\theta(x|z)$. Because,
        </p>
        <span>$$ p_\theta(x,z) = p_\theta(x|z)p_\theta(z) $$</span>
        <p>
          So $p_\theta(x,z)$ is easy to calculate for any given $x$ or $z$. The
          intractability of $p_\theta(x)$ stems from the integration operation
          in $(1)$. Because, the distribution $p_\theta(x|z)$ is usually
          characterized with neural networks, which makes the integration
          operation in $(1)$ intractable. Even if we assume the distributions
          $p_\theta(x|z)$ and $p_\theta(z)$ with known probability
          distributions, only in the case of conjugate pairs this integration is
          tractable. So, in most of the cases it is not possible to find an
          analytical solution to the integration. Also,
        </p>
        <span> $$p_\theta(x,z) = p_\theta(z|x)p_\theta(x)$$ </span>
        <span> $$p_\theta(x,z) = \frac{p_\theta(x,z)}{p_\theta(x)}$$ </span>
        <p>
          So, intractable $p_\theta(x)$ makes $p_\theta(z|x)$ intractable.
          Estimating $p_\theta (z|x)$ using variational distribution $q_\phi
          (z|x)$ is one solution.
          <strong>Our sole objective is to estimate</strong> $p_\theta (x)$. If
          $q_\varphi (z|x)$ is a close estimate of $p_\theta (z|x)$ we can
          estimate $p_\theta (x)$ by
        </p>
        <span> $$p_\theta(x) \sim \frac{p_\theta(x,z)}{q\phi(x|z)}$$ </span>
        <p>In fact $[3]$ uses a similar method (importance sampling).</p>
        <span>
          $$ log\space p_\theta(x) = log\space E_{q_\phi(z,x)}\left[
          \frac{p(x,z)}{q(x,z)} \right] $$
        </span>
        <p>
          There are several methods to variational inference (approximating the
          true posterior with $q(z|x)$).
        </p>
        <ul>
          <li>
            Comparatively older literature used to perform classical methods
            (MCMC, Mean field, conjugate priors etc.) to approximate $p(z|x)$
            with q(z|x).
          </li>
          <li>
            At around 2014 stochastic variational inference emerged where
            stochas- tic gradient descent is used by performing
            reparamitarization trick, which makes calculating gradients with
            respect to variational parameters possi- ble.
          </li>
          <li>
            Later at 2016 came normalization flow $[1]$ based variational
            inference.
          </li>
          <li>
            Recently adversarial techniques to estimate the posterior are
            becoming popular.
          </li>
        </ul>
        <p>
          Now getting back to our objective. We want to make the approximate
          poste- rior $q_\varphi (z|x)$ close to the true posterior $p_\theta
          (z|x)$. Essentially, we want to minimize the divergence between the
          two distributions.
        </p>
        <span>
          $$ \begin{align} D_{KL}\left[q_\phi(z|x)\space ||\space
          p_\theta(z|x)\right] &= E_{q_\phi(z|x)}\left[ log \left[
          \frac{q_\phi(z|x)}{p_\theta(z|x)} \right] \right]\notag\\
          &=E_{q_\phi(z|x)}\left[ log \left[
          \frac{q_\phi(z|x)p_\theta(x)}{p_\theta(z|x)p_\theta(x)} \right]
          \right]\notag\\ &=E_{q_\phi(z|x)}\left[ log \left[
          \frac{q_\phi(z|x)p_\theta(x)}{p_\theta(x,z)} \right] \right]\notag\\
          &=E_{q_\phi(z|x)}\left[ -log \left[
          \frac{p_\theta(x,z)}{q_\phi(z|x)p_\theta(x)} \right] \right]\notag\\
          &=E_{q_\phi(z|x)}log\space p_\theta(x) - E_{q_\phi(z|x)}\left[ log
          \left[ \frac{p_\theta(x,z)}{q_\phi(z|x)} \right] \right]\notag\\
          &=log\space p_\theta(x) - \underbrace{E_{q_\phi(z|x)}\left[ log\space
          p_\theta(x,z) - log\space q_\phi(z|x) \right]}_{ELBO}\tag{2}
          \end{align} $$
        </span>
      </div>
    </div>
    <div id="footer"></div>

    <!-- Le javascript
         ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <!-- <script src="js/jquery-1.9.1.min.js"></script> -->
    <script src="/js/bootstrap.min.js"></script>
    <script>
      $(document).ready(function () {
        $(document.body).scrollspy({
          target: "#navparent",
        });
      });
    </script>
    <script src="/js/post.js"></script>
  </body>
</html>
